{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "cMK0kw0myLFQ",
    "outputId": "41a4b414-ccff-4157-cd51-bba0cfcf6ed5"
   },
   "source": [
    "# Introduction\n",
    "In this notebook, we'll focus on data wrangling. Let's outline the steps and the goals of each step.\n",
    "\n",
    "|Step               |Goal                                                                   |\n",
    "|-------------------|-----------------------------------------------------------------------|\n",
    "|Data Collection    |Gather and join the data to streamline the next steps of the capstone  |\n",
    "|Data Organization: |Establish the file structure and version control                       |\n",
    "|Data Definition:   |Understand, annotate and clean the data in preparation for future work |\n",
    "|Data Cleaning:     |Check for missing data or wrong data, and handle them appropriately    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SmcYZLty2z8"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60Bl9Fvc9mho"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll import the data from Kaggle. \n",
    "\n",
    "Kaggle requires users to sign in and generate an API Key. Make sure your API key is at the correct location before running the next cell. If necessary, also make sure that Kaggle has been installed before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riiid-test-answer-prediction.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# Download the zipped data using the Kaggle API \n",
    "!kaggle competitions download -c riiid-test-answer-prediction -p \"..\\data\\raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip the downloaded file\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "zipped_data_path = Path('../data/raw/riiid-test-answer-prediction.zip')\n",
    "unzip_destination_folder_path = Path('../data/interim')\n",
    "\n",
    "with ZipFile(zipped_data_path, 'r') as zf:\n",
    "    # Save list of file names in zip file to a list\n",
    "    zf_names = zf.namelist()\n",
    "    # Extract all files\n",
    "    zf.extractall(unzip_destination_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example_sample_submission.csv',\n",
       " 'example_test.csv',\n",
       " 'lectures.csv',\n",
       " 'questions.csv',\n",
       " 'riiideducation/__init__.py',\n",
       " 'riiideducation/competition.cpython-37m-x86_64-linux-gnu.so',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the names of the unzipped files for the file names containing our data.\n",
    "zf_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_csv_files are  `lectures.csv`,  `questions.csv`, `train.csv` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mg3QVJ5x0KxU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Define data paths\n",
    "lectures_csv_path = Path('../data/interim/lectures.csv')\n",
    "questions_csv_path = Path('../data/interim/questions.csv')\n",
    "train_csv_path = Path('../data/interim/train.csv')\n",
    "\n",
    "# For these small csv files, import them directly with pandas.read_csv()\n",
    "df_lectures = pd.read_csv(lectures_csv_path)\n",
    "df_questions = pd.read_csv(questions_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train.csv` is a large csv file, over 7 GB and 100M rows, so we need to load it in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame generator from CSV in chunks\n",
    "df_generator = pd.read_csv(train_csv_path, chunksize=10000000)\n",
    "\n",
    "#Initialize an empty DataFrame: df_train\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "# Iterate over each DataFrame chunk\n",
    "for df_chunk in df_generator:\n",
    "    df_train = df_train.append(df_chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing as a dataframe, save the dataframe as a binary file, so that we can quickly reload the dataframe and resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BVwjKHLt8-o1"
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "lectures_pkl_path = Path('../data/interim/lectures.pkl.gzip')\n",
    "questions_pkl_path = Path('../data/interim/questions.pkl.gzip')\n",
    "train_pkl_path = Path('../data/interim/train.pkl.gzip')\n",
    "\n",
    "# Save DataFrames to as pkl\n",
    "df_lectures.to_pickle(lectures_pkl_path)\n",
    "df_questions.to_pickle(questions_pkl_path)\n",
    "df_train.to_pickle(train_pkl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the heads of the three DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>part</th>\n",
       "      <th>type_of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>159</td>\n",
       "      <td>5</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>solving question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>156</td>\n",
       "      <td>5</td>\n",
       "      <td>solving question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lecture_id  tag  part           type_of\n",
       "0          89  159     5           concept\n",
       "1         100   70     1           concept\n",
       "2         185   45     6           concept\n",
       "3         192   79     5  solving question\n",
       "4         317  156     5  solving question"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lectures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>bundle_id</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>part</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51 131 162 38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>131 36 81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 101 162 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131 149 162 29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>131 5 162 38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id  bundle_id  correct_answer  part            tags\n",
       "0            0          0               0     1   51 131 162 38\n",
       "1            1          1               1     1       131 36 81\n",
       "2            2          2               0     1  131 101 162 92\n",
       "3            3          3               0     1  131 149 162 29\n",
       "4            4          4               3     1    131 5 162 38"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>user_answer</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>5692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>56943</td>\n",
       "      <td>115</td>\n",
       "      <td>5716</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>118363</td>\n",
       "      <td>115</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>131167</td>\n",
       "      <td>115</td>\n",
       "      <td>7860</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>137965</td>\n",
       "      <td>115</td>\n",
       "      <td>7922</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  timestamp  user_id  content_id  content_type_id  task_container_id  \\\n",
       "0       0          0      115        5692                0                  1   \n",
       "1       1      56943      115        5716                0                  2   \n",
       "2       2     118363      115         128                0                  0   \n",
       "3       3     131167      115        7860                0                  3   \n",
       "4       4     137965      115        7922                0                  4   \n",
       "\n",
       "   user_answer  answered_correctly  prior_question_elapsed_time  \\\n",
       "0            3                   1                          NaN   \n",
       "1            2                   1                      37000.0   \n",
       "2            0                   1                      55000.0   \n",
       "3            0                   1                      19000.0   \n",
       "4            1                   1                      11000.0   \n",
       "\n",
       "  prior_question_had_explanation  \n",
       "0                            NaN  \n",
       "1                          False  \n",
       "2                          False  \n",
       "3                          False  \n",
       "4                          False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et-SnDv39rpq"
   },
   "source": [
    "## Data Joining\n",
    "\n",
    "The data is available as one large file. So their is no need to join the data other than joining the chunks of the large file while importing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Subsetting with the larger dataframes\n",
    "As the data is quite large, it might be useful to subset the data during exploratory data analysis to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the row skip logic\n",
    "\n",
    "#Skip rows from based on condition like skip every 10th line\n",
    "def skip_all_but_nth_rows(n, idx):\n",
    "  return (idx % n != 0)\n",
    "  \n",
    "#Skip random lines  \n",
    "import random\n",
    "def rand_1_in_n(n, idx):\n",
    "  return True if random.randrange(1,n)==1 else False\n",
    "\n",
    "\n",
    "#Create the subsets \n",
    "\n",
    "#Define a Dataframe with 1/10 of the data\n",
    "df_train_1_10 = df_train[df_train.index % 10 == 0]\n",
    "\n",
    "#Define a DataFrame with 1/100 of the data\n",
    "df_train_1_100 = df_train[df_train.index % 100 == 0]\n",
    "\n",
    "#Define a DataFrame with 1/1000 of the data\n",
    "df_train_1_1000 = df_train[df_train.index % 1000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/10000 of the data\n",
    "df_train_1_10000 = df_train[df_train.index % 10000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/100000 of the data\n",
    "df_train_1_100000 = df_train[df_train.index % 100000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/1000000 of the data\n",
    "df_train_1_1000000 = df_train[df_train.index % 1000000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/10000000 of the data\n",
    "df_train_1_10000000 = df_train[df_train.index % 10000000 == 0]\n",
    "\n",
    "\n",
    "#Define subset paths\n",
    "train_pkl_path_1_10 = Path('../data/interim/train_1_10.pkl.gzip')\n",
    "train_pkl_path_1_100 = Path('../data/interim/train_1_100.pkl.gzip')\n",
    "train_pkl_path_1_1000 = Path('../data/interim/train_1_1000.pkl.gzip')\n",
    "train_pkl_path_1_10000 = Path('../data/interim/train_1_10000.pkl.gzip')\n",
    "train_pkl_path_1_100000 = Path('../data/interim/train_1_100000.pkl.gzip')\n",
    "train_pkl_path_1_1000000 = Path('../data/interim/train_1_1000000.pkl.gzip')\n",
    "train_pkl_path_1_10000000 = Path('../data/interim/train_1_10000000.pkl.gzip')\n",
    "\n",
    "#Save subset dataframes to pkl\n",
    "df_train_1_10.to_pickle(train_pkl_path_1_10)\n",
    "df_train_1_100.to_pickle(train_pkl_path_1_100)\n",
    "df_train_1_1000.to_pickle(train_pkl_path_1_1000)\n",
    "df_train_1_10000.to_pickle(train_pkl_path_1_10000)\n",
    "df_train_1_100000.to_pickle(train_pkl_path_1_100000)\n",
    "df_train_1_1000000.to_pickle(train_pkl_path_1_1000000)\n",
    "df_train_1_10000000.to_pickle(train_pkl_path_1_10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming data analysis without reimporting data from source files\n",
    "After the binary files have been saved, we can quickly resume by loading the binary files rather tha downloading, unzipping, and reading the csv files in chunks, again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((418, 4), (13523, 5), (101230332, 10))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "lectures_pkl_path = Path('../data/interim/lectures.pkl.gzip')\n",
    "questions_pkl_path = Path('../data/interim/questions.pkl.gzip')\n",
    "train_pkl_path = Path('../data/interim/train.pkl.gzip')\n",
    "\n",
    "with open(lectures_pkl_path, 'rb') as f:\n",
    "    df_lectures = pickle.load(f)\n",
    "    \n",
    "with open(questions_pkl_path, 'rb') as f:\n",
    "    df_questions = pickle.load(f)\n",
    "    \n",
    "with open(train_pkl_path, 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "    \n",
    "# Check the shape of the dataframes\n",
    "df_lectures.shape, df_questions.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the subsets from the pkl files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10123034, 10), (1012304, 10), (101231, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define subset paths\n",
    "train_pkl_path_1_10 = Path('../data/interim/train_1_10.pkl.gzip')\n",
    "train_pkl_path_1_100 = Path('../data/interim/train_1_100.pkl.gzip')\n",
    "train_pkl_path_1_1000 = Path('../data/interim/train_1_1000.pkl.gzip')\n",
    "train_pkl_path_1_10000 = Path('../data/interim/train_1_10000.pkl.gzip')\n",
    "train_pkl_path_1_100000 = Path('../data/interim/train_1_100000.pkl.gzip')\n",
    "train_pkl_path_1_1000000 = Path('../data/interim/train_1_1000000.pkl.gzip')\n",
    "train_pkl_path_1_10000000 = Path('../data/interim/train_1_10000000.pkl.gzip')\n",
    "\n",
    "#Load the subsets\n",
    "with open( train_pkl_path_1_10, 'rb') as f:\n",
    "  df_train_1_10 = pickle.load(f)\n",
    "with open( train_pkl_path_1_100, 'rb') as f:\n",
    "  df_train_1_100 = pickle.load(f)\n",
    "with open( train_pkl_path_1_1000, 'rb') as f:\n",
    "  df_train_1_1000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_10000, 'rb') as f:\n",
    "  df_train_1_10000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_100000, 'rb') as f:\n",
    "  df_train_1_100000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_1000000, 'rb') as f:\n",
    "  df_train_1_1000000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_10000000, 'rb') as f:\n",
    "  df_train_1_10000000 = pickle.load(f)\n",
    "\n",
    "# Store a list of the subset DataFrames\n",
    "df_train_subsets = [df_train_1_10, \n",
    "                    df_train_1_100, \n",
    "                    df_train_1_1000,\n",
    "                    df_train_1_10000,\n",
    "                    df_train_1_100000,\n",
    "                    df_train_1_1000000,\n",
    "                    df_train_1_10000000,]\n",
    "\n",
    "# Check the shape of the subset dataframes\n",
    "df_train_1_10.shape, df_train_1_100.shape, df_train_1_1000.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGc2jua1y44z"
   },
   "source": [
    "# Data Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TdU2Vg0y7ek"
   },
   "source": [
    "## File Structure\n",
    "\n",
    "We'll use the default [file structure template for data science from cookiecutter data science](https://medium.com/@rrfd/cookiecutter-data-science-organize-your-projects-atom-and-jupyter-2be7862f487e).\n",
    "\n",
    "The data files have already been imported according to this template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FxQEMlbTb9p"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "|             |                  |\n",
    "|-------------|------------------|\n",
    "|├── README.md|          <- Front page of the project. Let everyone |\n",
    "|│|                         know the major points.|\n",
    "|│|\n",
    "|├── models|             <- Trained and serialized models, model|\n",
    "|│|                         predictions, or model summaries.|\n",
    "|│|\n",
    "|├── notebooks|          <- Jupyter notebooks. Use set naming|\n",
    "|│|                         E.g. `1.2-rd-data-exploration`.|\n",
    "|│|\n",
    "|├── reports|            <- HTML, PDF, and LaTeX.|\n",
    "|│   └── figures|        <- Generated figures.|\n",
    "|│|\n",
    "|├── requirements.txt|   <- File for reproducing the environment|\n",
    "|│|                         `$ pip freeze > requirements.txt`|\n",
    "|├── data|\n",
    "|│   ├── external|       <- Third party sources.|\n",
    "|│   ├── interim|        <- In-progress intermediate data.|\n",
    "|│   ├── processed|      <- The final data sets for modelling.|\n",
    "|│   └── raw|            <- The original, immutable data.|\n",
    "|│|\n",
    "|└── src |               <- Source code for use in this project.|\n",
    "|    ├── __init__.py|    <- Makes src a Python module. |\n",
    "|    │|\n",
    "|    ├── custom_func.py| <- Various custom functions to import.|\n",
    "|    │|\n",
    "|    ├── data          | <- Scripts to download or generate data.|\n",
    "|    │   └── make_dataset.py|\n",
    "|    │|\n",
    "|    ├── features|       <- Scripts raw data into features for|\n",
    "|    │   │        |         modeling.|\n",
    "|    │   └── build_features.py|\n",
    "|    │|\n",
    "|    ├── models|         <- Scripts to train models and then use|\n",
    "|    │   │     |            trained models to make predictions.|\n",
    "|    │   │     |            \n",
    "|    │   ├── predict_model.py|\n",
    "|    │   └── train_model.py|\n",
    "|    │|\n",
    "|    └── viz|            <- Scripts to create visualizations.|            \n",
    "|        └── viz.py|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Control\n",
    "\n",
    "This notebook and it's related files will be stored in a local repository and on Github at:\n",
    "https://github.com/allen44/capstone-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental variables\n",
    "Following the best practices outlined in the [Twelve Factor App](https://12factor.net/), environmental variables will be excluded from version control.\n",
    "\n",
    "FOr this notebook, that means that any user wishing to reproduce the data loading steps will need their own Kaggle API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kSKgl3FzAIu"
   },
   "source": [
    "# Data Definition\n",
    "\n",
    "Kaggle lists the definitions of the data on the [competition webpage](https://www.kaggle.com/c/riiid-test-answer-prediction/data).\n",
    "\n",
    "Here's a excerpt of the relevant section:\n",
    "\n",
    "\n",
    "### lectures.csv: metadata for the lectures watched by users as they progress in their education.\n",
    ">`lecture_id`: foreign key for the train/test content_id column, when the content type is lecture (1).\n",
    "\n",
    ">`part`: top level category code for the lecture.\n",
    "\n",
    ">`tag`: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n",
    "\n",
    ">`type_of`: brief description of the core purpose of the lecture\n",
    "\n",
    "### questions.csv: metadata for the questions posed to users.\n",
    ">`question_id`: foreign key for the train/test content_id column, when the content type is question (0).\n",
    "\n",
    ">`bundle_id`: code for which questions are served together.\n",
    "\n",
    ">`correct_answer`: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n",
    "\n",
    ">`part`: the relevant section of the TOEIC test.\n",
    "\n",
    ">`tags`: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n",
    "\n",
    "### train.csv \n",
    ">`content_id`: (int16) ID code for the user interaction\n",
    "\n",
    ">`content_type_id`: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n",
    "\n",
    ">`task_container_id`: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n",
    "\n",
    ">`user_answer`: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n",
    "\n",
    ">`answered_correctly`: (int8) if the user responded correctly. Read -1 as null, for lectures.\n",
    "\n",
    ">`prior_question_elapsed_time`: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n",
    "\n",
    ">`prior_question_had_explanation`: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dtypes\n",
    "\n",
    "Based on the Kaggle eplanatoins of the data columns, we have enough info to set the dtypes on the DataFrames, and label missing or null data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_lectures\n",
      " lecture_id     int64\n",
      "tag            int64\n",
      "part           int64\n",
      "type_of       object\n",
      "dtype: object \n",
      "\n",
      "df_questions\n",
      " question_id        int64\n",
      "bundle_id          int64\n",
      "correct_answer     int64\n",
      "part               int64\n",
      "tags              object\n",
      "dtype: object \n",
      "\n",
      "df_train\n",
      " row_id                              int64\n",
      "timestamp                           int64\n",
      "user_id                             int64\n",
      "content_id                          int64\n",
      "content_type_id                     int64\n",
      "task_container_id                   int64\n",
      "user_answer                         int64\n",
      "answered_correctly                  int64\n",
      "prior_question_elapsed_time       float64\n",
      "prior_question_had_explanation     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Check current dtypes\n",
    "print('df_lectures\\n', df_lectures.dtypes, '\\n')\n",
    "print('df_questions\\n', df_questions.dtypes,  '\\n')\n",
    "print('df_train\\n', df_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dtypes don't match the data as described by the Kaggle data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dtypes\n",
    "lectures_dtypes = {'lecture_id': 'category',\n",
    "                    'part': 'category',\n",
    "                    'tag': 'category',\n",
    "                    'type_of': 'string'}\n",
    "            \n",
    "questions_dtypes = {'question_id': 'category', \n",
    "                   'bundle_id': 'category',\n",
    "                   'correct_answer': 'category', \n",
    "                   'part': 'category',\n",
    "                   'tags': 'category'}\n",
    "            \n",
    "train_dtypes = {'row_id': 'category',\n",
    "                'timestamp': 'int64',\n",
    "                'user_id': 'category',\n",
    "                'content_id': 'category', \n",
    "                'content_type_id': 'category',\n",
    "                'task_container_id': 'category', \n",
    "                'user_answer': 'category',\n",
    "                'answered_correctly': 'category',\n",
    "                'prior_question_elapsed_time': 'float',\n",
    "                'prior_question_had_explanation': 'category'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_lectures\n",
      " lecture_id    category\n",
      "tag           category\n",
      "part          category\n",
      "type_of         string\n",
      "dtype: object \n",
      "\n",
      "df_questions\n",
      " question_id       category\n",
      "bundle_id         category\n",
      "correct_answer    category\n",
      "part              category\n",
      "tags              category\n",
      "dtype: object \n",
      "\n",
      "df_train\n",
      " row_id                            category\n",
      "timestamp                            int64\n",
      "user_id                           category\n",
      "content_id                        category\n",
      "content_type_id                   category\n",
      "task_container_id                 category\n",
      "user_answer                       category\n",
      "answered_correctly                category\n",
      "prior_question_elapsed_time        float64\n",
      "prior_question_had_explanation    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Set the dtypes\n",
    "df_lectures = df_lectures.astype(lectures_dtypes)\n",
    "df_questions = df_questions.astype(questions_dtypes)\n",
    "df_train = df_train.astype(train_dtypes)\n",
    "\n",
    "#Check the new dtypes\n",
    "print('df_lectures\\n', df_lectures.dtypes, '\\n')\n",
    "print('df_questions\\n', df_questions.dtypes,  '\\n')\n",
    "print('df_train\\n', df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_1_10: \n",
      " row_id                            category\n",
      "timestamp                            int64\n",
      "user_id                           category\n",
      "content_id                        category\n",
      "content_type_id                   category\n",
      "task_container_id                 category\n",
      "user_answer                       category\n",
      "answered_correctly                category\n",
      "prior_question_elapsed_time        float64\n",
      "prior_question_had_explanation    category\n",
      "dtype: object\n",
      "df_train_1_1000: \n",
      " row_id                            category\n",
      "timestamp                            int64\n",
      "user_id                           category\n",
      "content_id                        category\n",
      "content_type_id                   category\n",
      "task_container_id                 category\n",
      "user_answer                       category\n",
      "answered_correctly                category\n",
      "prior_question_elapsed_time        float64\n",
      "prior_question_had_explanation    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Also, set the dtypes on train subset DataFrames\n",
    "df_train_1_10 = df_train_1_10.astype(train_dtypes)\n",
    "df_train_1_100 = df_train_1_100.astype(train_dtypes)\n",
    "df_train_1_1000 = df_train_1_1000.astype(train_dtypes)\n",
    "df_train_1_10000 = df_train_1_10000.astype(train_dtypes)\n",
    "df_train_1_100000 = df_train_1_100000.astype(train_dtypes)\n",
    "df_train_1_1000000 = df_train_1_1000000.astype(train_dtypes)\n",
    "df_train_1_10000000 = df_train_1_10000000.astype(train_dtypes)\n",
    "\n",
    "#Check dtypes\n",
    "print('df_train_1_10: \\n', df_train_1_10.dtypes)\n",
    "print('df_train_1_1000: \\n', df_train_1_1000.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the original columns\n",
    "\n",
    "We'll add columns later, so we'll save lists of the original columns now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_id',\n",
       " 'timestamp',\n",
       " 'user_id',\n",
       " 'content_id',\n",
       " 'content_type_id',\n",
       " 'task_container_id',\n",
       " 'user_answer',\n",
       " 'answered_correctly',\n",
       " 'prior_question_elapsed_time',\n",
       " 'prior_question_had_explanation']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save original columns in lists\n",
    "\n",
    "lectures_columns_original = list(df_lectures.columns)\n",
    "questions_columns_original = list(df_questions.columns)\n",
    "train_columns_original = list(df_train.columns)\n",
    "\n",
    "#Check the original coliumns\n",
    "lectures_columns_original, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the set of unique tags, find number of unique tags\n",
    "\n",
    "`tags` is a compound variable.\n",
    "\n",
    "The entries in the `tags` column in `df_questions` are numerical ids seperated by spaces where each id is a category that corresponds to the subject matter of the question. Many questions have more than one id in `tags`, though some questions only have one. \n",
    "\n",
    "We will eventually to use `Dataframe.get_dummy()` to seperate the compound variable. For now, let's define the set of tags and count the unique tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [51, 131, 162, 38]\n",
       "1              [131, 36, 81]\n",
       "2        [131, 101, 162, 92]\n",
       "3        [131, 149, 162, 29]\n",
       "4          [131, 5, 162, 38]\n",
       "                ...         \n",
       "13518                   [14]\n",
       "13519                    [8]\n",
       "13520                   [73]\n",
       "13521                  [125]\n",
       "13522                   [55]\n",
       "Name: tags, Length: 13523, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use pandas string methods to convert the string to a list of ids\n",
    "tags = df_questions['tags'].str.split()\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '11',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '14',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '15',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '16',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '167',\n",
       " '168',\n",
       " '169',\n",
       " '17',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '174',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '178',\n",
       " '179',\n",
       " '18',\n",
       " '180',\n",
       " '181',\n",
       " '182',\n",
       " '183',\n",
       " '184',\n",
       " '185',\n",
       " '186',\n",
       " '187',\n",
       " '19',\n",
       " '2',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '3',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '4',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '5',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '6',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '7',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '8',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '9',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " nan}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the set of unique tags\n",
    "set_of_tags = set(tags.explode().unique())\n",
    "\n",
    "# Check the set of unique tags\n",
    "set_of_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique tags\n",
    "len(set_of_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Profiling Report\n",
    "The Pandas Profiling module is a quick way to get an overview of the data sets. For the largest dataset, we will make a profile report on the subset only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.2 TiB for an array with shape (5616439704540,) and data type int16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-f2748b61519b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#Save reports to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mreport_lectures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpandas_profiling_report_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'lectures.html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mreport_questions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpandas_profiling_report_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'questions.html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mreport_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpandas_profiling_report_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'train_tmp.html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\profile_report.py\u001b[0m in \u001b[0;36mto_file\u001b[1;34m(self, output_file, silent)\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0mcreate_html_assets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\".html\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\profile_report.py\u001b[0m in \u001b[0;36mto_html\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \"\"\"\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\profile_report.py\u001b[0m in \u001b[0;36mhtml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_html\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\profile_report.py\u001b[0m in \u001b[0;36m_render_html\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpresentation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflavours\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTMLReport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[0mdisable_progress_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"progress_bar\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\profile_report.py\u001b[0m in \u001b[0;36mreport\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_report_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescription_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\profile_report.py\u001b[0m in \u001b[0;36mdescription_set\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_description_set\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             self._description_set = describe_df(\n\u001b[1;32m--> 171\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m             )\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_description_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\model\\describe.py\u001b[0m in \u001b[0;36mdescribe\u001b[1;34m(title, df, summarizer, typeset, sample)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m# Duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Locating duplicates\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mduplicates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupported_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas_profiling\\model\\duplicates.py\u001b[0m in \u001b[0;36mget_duplicates\u001b[1;34m(df, supported_columns)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return (\n\u001b[0;32m     22\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msupported_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupported_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36msize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1639\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1641\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1643\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_reindex_output\u001b[1;34m(self, output, fill_value)\u001b[0m\n\u001b[0;32m   2886\u001b[0m         \u001b[0mlevels_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_index\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mping\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroupings\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2887\u001b[0m         index, _ = MultiIndex.from_product(\n\u001b[1;32m-> 2888\u001b[1;33m             \u001b[0mlevels_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2889\u001b[0m         ).sortlevel()\n\u001b[0;32m   2890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\multi.py\u001b[0m in \u001b[0;36mfrom_product\u001b[1;34m(cls, iterables, sortorder, names)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;31m# codes are all ndarrays, so cartesian_product is lossless\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         \u001b[0mcodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcartesian_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msortorder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msortorder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\util.py\u001b[0m in \u001b[0;36mcartesian_product\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcumprodX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtile_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcumprodX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtile_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(a, repeats, axis)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m     \"\"\"\n\u001b[1;32m--> 479\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'repeat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.2 TiB for an array with shape (5616439704540,) and data type int16"
     ]
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "from pathlib import Path\n",
    "\n",
    "#Define Pandas Profile Report save path\n",
    "pandas_profiling_report_dir = Path('../reports')\n",
    "\n",
    "#Use a subset of df_train\n",
    "df_train_tmp = df_train_1_1000000\n",
    "\n",
    "#Generate Pandas Profiling Report - may be slow running if using the largest subsets of the data\n",
    "\n",
    "report_lectures= ProfileReport(df_lectures, sort='None', html={'style':{'full_width': True}}, progress_bar=False)\n",
    "report_questions = ProfileReport(df_questions, sort='None', html={'style':{'full_width': True}}, progress_bar=False)\n",
    "report_train= ProfileReport(df_train_tmp, sort='None', html={'style':{'full_width': True}}, progress_bar=False)\n",
    "\n",
    "del df_train_tmp\n",
    "\n",
    "#Save reports to file\n",
    "report_lectures.to_file(pandas_profiling_report_dir / 'lectures.html')\n",
    "report_questions.to_file(pandas_profiling_report_dir / 'questions.html')\n",
    "report_train.to_file(pandas_profiling_report_dir / 'train_tmp.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Pandas Profiling reports, we can see that the data is pretty clean, as expected for a Kaggle dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtXv1bctzDw8"
   },
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lectures.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `lectures` has no missing data, `questions` has one missing tag, and `train` has 2 million (about 2%) missing `prior_question_elapsed_time` entries and less than 1% missing entries in `prior_question_had_explanation`. Based on the explanation of the data from Kaggle, we can conclude that this missing data is normal. Nothing should be discard or imputed at this stage.\n",
    "\n",
    "Note that the target variable, `answered_correctly`, has no missing data. This is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraction of duplicates in df_lectures:\")\n",
    "df_lectures.duplicated().sum() / len(df_lectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraction of duplicates in df_questions:\")\n",
    "df_questions.duplicated().sum() / len(df_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraction of duplicates in df_train (df_train_1_100):\")\n",
    "df_train_1_100.duplicated().sum() / len(df_train_1_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as this Kaggle competition dataset, which are known to be fairly clean, there is no duplicated entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitue \"NA\" for the '-1' values \n",
    "\n",
    "Two columns in df_train, `user_answer` and `answered_correctly`, have `-1` values for null. Let's substitute `pd.NaN` for the null values instead.\n",
    "\n",
    "After, we'll create two boolean columns, `user_answer_was_-1` and `answered_correctly_was_-1`, to denote when the values in the colums were changed during data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two boolean columns to note the initial state of the uncleaned columns\n",
    "df_train['user_answer_was_-1'] = True if df_train['user_answer'] == -1 else False \n",
    "df_train['answered_correctly_was_-1'] = True if df_train['answered_correctly'] == -1 else False\n",
    "\n",
    "# Substitue \"NA\" for the '-1' values\n",
    "df_train.loc[['user_answer', 'answered_correctly']] = df_train.loc[:, ['user_answer', 'answered_correctly']].replace(-1, pd.NaN)\n",
    "\n",
    "#Check the new columns\n",
    "df_train.loc[['user_answer', 'user_answer_was_-1',  'answered_correctly',  'answered_correctly_was_-1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Capstone 2 - Data Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
