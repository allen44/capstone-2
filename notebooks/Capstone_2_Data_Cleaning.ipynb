{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "cMK0kw0myLFQ",
    "outputId": "41a4b414-ccff-4157-cd51-bba0cfcf6ed5"
   },
   "source": [
    "# Introduction\n",
    "In this notebook, we'll focus on data wrangling. Let's outline the steps and the goals of each step.\n",
    "\n",
    "|Step               |Goal                                                                   |\n",
    "|-------------------|-----------------------------------------------------------------------|\n",
    "|Data Collection    |Gather and join the data to streamline the next steps of the capstone  |\n",
    "|Data Organization: |Establish the file structure and version control                       |\n",
    "|Data Definition:   |Understand, annotate and clean the data in preparation for future work |\n",
    "|Data Cleaning:     |Check for missing data or wrong data, and handle them appropriately    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SmcYZLty2z8"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60Bl9Fvc9mho"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll import the data from Kaggle. \n",
    "\n",
    "Kaggle requires users to sign in and generate an API Key. Make sure your API key is at the correct location before running the next cell. If necessary, also make sure that Kaggle has been installed before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riiid-test-answer-prediction.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# Download the zipped data using the Kaggle API \n",
    "!kaggle competitions download -c riiid-test-answer-prediction -p \"..\\data\\raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip the downloaded file\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "zipped_data_path = Path('../data/raw/riiid-test-answer-prediction.zip')\n",
    "unzip_destination_folder_path = Path('../data/interim')\n",
    "\n",
    "with ZipFile(zipped_data_path, 'r') as zf:\n",
    "    # Save list of file names in zip file to a list\n",
    "    zf_names = zf.namelist()\n",
    "    # Extract all files\n",
    "    zf.extractall(unzip_destination_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example_sample_submission.csv',\n",
       " 'example_test.csv',\n",
       " 'lectures.csv',\n",
       " 'questions.csv',\n",
       " 'riiideducation/__init__.py',\n",
       " 'riiideducation/competition.cpython-37m-x86_64-linux-gnu.so',\n",
       " 'train.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the names of the unzipped files for the file names containing our data.\n",
    "zf_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_csv_files are  `lectures.csv`,  `questions.csv`, `train.csv` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mg3QVJ5x0KxU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Define data paths\n",
    "lectures_csv_path = Path('../data/interim/lectures.csv')\n",
    "questions_csv_path = Path('../data/interim/questions.csv')\n",
    "train_csv_path = Path('../data/interim/train.csv')\n",
    "\n",
    "# For these small csv files, import them directly with pandas.read_csv()\n",
    "df_lectures = pd.read_csv(lectures_csv_path)\n",
    "df_questions = pd.read_csv(questions_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train.csv` is a large csv file, over 7 GB and 100M rows, so we need to load it in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame generator from CSV in chunks\n",
    "df_generator = pd.read_csv(train_csv_path, chunksize=10000000)\n",
    "\n",
    "#Initialize an empty DataFrame: df_train\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "# Iterate over each DataFrame chunk\n",
    "for df_chunk in df_generator:\n",
    "    df_train = df.append(df_chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing as a dataframe, save the dataframe as a binary file, so that we can quickly reload the dataframe and resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVwjKHLt8-o1"
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "lectures_pkl_path = Path('../data/interim/lectures.pkl.gzip')\n",
    "questions_pkl_path = Path('../data/interim/questions.pkl.gzip')\n",
    "train_pkl_path = Path('../data/interim/train.pkl.gzip')\n",
    "\n",
    "# Save DataFrames to as pkl\n",
    "df_lectures.to_pickle(lectures_pkl_path)\n",
    "df_questions.to_pickle(questions_pkl_path)\n",
    "df_train.to_pickle(train_pkl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et-SnDv39rpq"
   },
   "source": [
    "## Data Joining\n",
    "\n",
    "The data is available as one large file. So their is no need to join the data other than joining the chunks of the large file while importing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Subsetting with the larger dataframes\n",
    "As the data is quite large, it might be useful to subset the data during exploratory data analysis to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the row skip logic\n",
    "\n",
    "#Skip rows from based on condition like skip every 10th line\n",
    "def skip_all_but_nth_rows(n, idx):\n",
    "  return (idx % n != 0)\n",
    "  \n",
    "#Skip random lines  \n",
    "import random\n",
    "def rand_1_in_n(n, idx):\n",
    "  return True if random.randrange(1,n)==1 else False\n",
    "\n",
    "\n",
    "#Create the subsets \n",
    "\n",
    "#Define a Dataframe with 1/10 of the data\n",
    "df_train_1_10 = df_train[df_train.index % 10 == 0]\n",
    "\n",
    "#Define a DataFrame with 1/100 of the data\n",
    "df_train_1_100 = df_train[df_train.index % 100 == 0]\n",
    "\n",
    "#Define a DataFrame with 1/1000 of the data\n",
    "df_train_1_1000 = df_train[df_train.index % 1000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/10000 of the data\n",
    "df_train_1_10000 = df_train[df_train.index % 10000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/100000 of the data\n",
    "df_train_1_100000 = df_train[df_train.index % 100000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/1000000 of the data\n",
    "df_train_1_1000000 = df_train[df_train.index % 1000000 == 0]\n",
    "\n",
    "#Define a Dataframe with 1/10000000 of the data\n",
    "df_train_1_10000000 = df_train[df_train.index % 10000000 == 0]\n",
    "\n",
    "\n",
    "#Define subset paths\n",
    "train_pkl_path_1_10 = Path('../data/interim/train_1_10.pkl.gzip')\n",
    "train_pkl_path_1_100 = Path('../data/interim/train_1_100.pkl.gzip')\n",
    "train_pkl_path_1_1000 = Path('../data/interim/train_1_1000.pkl.gzip')\n",
    "train_pkl_path_1_10000 = Path('../data/interim/train_1_10000.pkl.gzip')\n",
    "train_pkl_path_1_100000 = Path('../data/interim/train_1_100000.pkl.gzip')\n",
    "train_pkl_path_1_1000000 = Path('../data/interim/train_1_1000000.pkl.gzip')\n",
    "train_pkl_path_1_10000000 = Path('../data/interim/train_1_10000000.pkl.gzip')\n",
    "\n",
    "#Save subset dataframes to pkl\n",
    "df_train_1_10.to_pickle(train_pkl_path_1_10)\n",
    "df_train_1_100.to_pickle(train_pkl_path_1_100)\n",
    "df_train_1_1000.to_pickle(train_pkl_path_1_1000)\n",
    "df_train_1_10000.to_pickle(train_pkl_path_1_10000)\n",
    "df_train_1_100000.to_pickle(train_pkl_path_1_100000)\n",
    "df_train_1_1000000.to_pickle(train_pkl_path_1_1000000)\n",
    "df_train_1_10000000.to_pickle(train_pkl_path_1_10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming data analysis without reimporting data from source files\n",
    "After the binary files have been saved, we can quickly resume by loading the binary files rather tha downloading, unzipping, and reading the csv files in chunks, again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((418, 4), (13523, 5), (101230332, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "lectures_pkl_path = Path('../data/interim/lectures.pkl.gzip')\n",
    "questions_pkl_path = Path('../data/interim/questions.pkl.gzip')\n",
    "train_pkl_path = Path('../data/interim/train.pkl.gzip')\n",
    "\n",
    "with open(lectures_pkl_path, 'rb') as f:\n",
    "    df_lectures = pickle.load(f)\n",
    "    \n",
    "with open(questions_pkl_path, 'rb') as f:\n",
    "    df_questions = pickle.load(f)\n",
    "    \n",
    "with open(train_pkl_path, 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "    \n",
    "# Check the shape of the dataframes\n",
    "df_lectures.shape, df_questions.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the subsets from the pkl files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10123034, 9), (1012304, 9), (101231, 9))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define subset paths\n",
    "train_pkl_path_1_10 = Path('../data/interim/train_1_10.pkl.gzip')\n",
    "train_pkl_path_1_100 = Path('../data/interim/train_1_100.pkl.gzip')\n",
    "train_pkl_path_1_1000 = Path('../data/interim/train_1_1000.pkl.gzip')\n",
    "train_pkl_path_1_10000 = Path('../data/interim/train_1_10000.pkl.gzip')\n",
    "train_pkl_path_1_100000 = Path('../data/interim/train_1_100000.pkl.gzip')\n",
    "train_pkl_path_1_1000000 = Path('../data/interim/train_1_1000000.pkl.gzip')\n",
    "train_pkl_path_1_10000000 = Path('../data/interim/train_1_10000000.pkl.gzip')\n",
    "\n",
    "#Load the subsets\n",
    "with open( train_pkl_path_1_10, 'rb') as f:\n",
    "  df_train_1_10 = pickle.load(f)\n",
    "with open( train_pkl_path_1_100, 'rb') as f:\n",
    "  df_train_1_100 = pickle.load(f)\n",
    "with open( train_pkl_path_1_1000, 'rb') as f:\n",
    "  df_train_1_1000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_10000, 'rb') as f:\n",
    "  df_train_1_10000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_100000, 'rb') as f:\n",
    "  df_train_1_100000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_1000000, 'rb') as f:\n",
    "  df_train_1_1000000 = pickle.load(f)\n",
    "with open( train_pkl_path_1_10000000, 'rb') as f:\n",
    "  df_train_1_10000000 = pickle.load(f)\n",
    "\n",
    "# Check the shape of the subset dataframes\n",
    "df_train_1_10.shape, df_train_1_100.shape, df_train_1_1000.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGc2jua1y44z"
   },
   "source": [
    "# Data Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TdU2Vg0y7ek"
   },
   "source": [
    "## File Structure\n",
    "\n",
    "We'll use the default [file structure template for data science from cookiecutter data science](https://medium.com/@rrfd/cookiecutter-data-science-organize-your-projects-atom-and-jupyter-2be7862f487e).\n",
    "\n",
    "The data files have already been imported according to this template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FxQEMlbTb9p"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "|             |                  |\n",
    "|-------------|------------------|\n",
    "|├── README.md|          <- Front page of the project. Let everyone |\n",
    "|│|                         know the major points.|\n",
    "|│|\n",
    "|├── models|             <- Trained and serialized models, model|\n",
    "|│|                         predictions, or model summaries.|\n",
    "|│|\n",
    "|├── notebooks|          <- Jupyter notebooks. Use set naming|\n",
    "|│|                         E.g. `1.2-rd-data-exploration`.|\n",
    "|│|\n",
    "|├── reports|            <- HTML, PDF, and LaTeX.|\n",
    "|│   └── figures|        <- Generated figures.|\n",
    "|│|\n",
    "|├── requirements.txt|   <- File for reproducing the environment|\n",
    "|│|                         `$ pip freeze > requirements.txt`|\n",
    "|├── data|\n",
    "|│   ├── external|       <- Third party sources.|\n",
    "|│   ├── interim|        <- In-progress intermediate data.|\n",
    "|│   ├── processed|      <- The final data sets for modelling.|\n",
    "|│   └── raw|            <- The original, immutable data.|\n",
    "|│|\n",
    "|└── src |               <- Source code for use in this project.|\n",
    "|    ├── __init__.py|    <- Makes src a Python module. |\n",
    "|    │|\n",
    "|    ├── custom_func.py| <- Various custom functions to import.|\n",
    "|    │|\n",
    "|    ├── data          | <- Scripts to download or generate data.|\n",
    "|    │   └── make_dataset.py|\n",
    "|    │|\n",
    "|    ├── features|       <- Scripts raw data into features for|\n",
    "|    │   │        |         modeling.|\n",
    "|    │   └── build_features.py|\n",
    "|    │|\n",
    "|    ├── models|         <- Scripts to train models and then use|\n",
    "|    │   │     |            trained models to make predictions.|\n",
    "|    │   │     |            \n",
    "|    │   ├── predict_model.py|\n",
    "|    │   └── train_model.py|\n",
    "|    │|\n",
    "|    └── viz|            <- Scripts to create visualizations.|            \n",
    "|        └── viz.py|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version Control\n",
    "\n",
    "This notebook and it's related files will be stored in a local repository and on Github at:\n",
    "https://github.com/allen44/capstone-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental variables\n",
    "Following the best practices outlined in the [Twelve Factor App](https://12factor.net/), environmental variables will be excluded from version control.\n",
    "\n",
    "FOr this notebook, that means that any user wishing to reproduce the data loading steps will need their own Kaggle API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kSKgl3FzAIu"
   },
   "source": [
    "# Data Definition\n",
    "\n",
    "Kaggle lists the definitions of the data on the [competition webpage](https://www.kaggle.com/c/riiid-test-answer-prediction/data).\n",
    "\n",
    "Here's a excerpt of the relevant section:\n",
    "\n",
    "\n",
    "### lectures.csv: metadata for the lectures watched by users as they progress in their education.\n",
    ">`lecture_id`: foreign key for the train/test content_id column, when the content type is lecture (1).\n",
    "\n",
    ">`part`: top level category code for the lecture.\n",
    "\n",
    ">`tag`: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n",
    "\n",
    ">`type_of`: brief description of the core purpose of the lecture\n",
    "\n",
    "### questions.csv: metadata for the questions posed to users.\n",
    ">`question_id`: foreign key for the train/test content_id column, when the content type is question (0).\n",
    "\n",
    ">`bundle_id`: code for which questions are served together.\n",
    "\n",
    ">`correct_answer`: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n",
    "\n",
    ">`part`: the relevant section of the TOEIC test.\n",
    "\n",
    ">`tags`: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.\n",
    "\n",
    "### train.csv \n",
    ">`content_id`: (int16) ID code for the user interaction\n",
    "\n",
    ">`content_type_id`: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n",
    "\n",
    ">`task_container_id`: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n",
    "\n",
    ">`user_answer`: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n",
    "\n",
    ">`answered_correctly`: (int8) if the user responded correctly. Read -1 as null, for lectures.\n",
    "\n",
    ">`prior_question_elapsed_time`: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n",
    "\n",
    ">`prior_question_had_explanation`: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtXv1bctzDw8"
   },
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Profiling\n",
    "The Pandas Profiling module is a quick way to get an overview of the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install updated pandas profiling\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "%autoreload 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAYmg1WtyVNx"
   },
   "source": [
    "Based on the excerpt above, we have enough info to set the dtypes on the DataFrames, and label missing or null data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_dtypes = {'lecture_id': 'category',\n",
    "                    'part': 'category',\n",
    "                    'tag': 'category',\n",
    "                    'type_of': 'string'}\n",
    "            \n",
    "questions_dtypes = {'question_id': 'category', \n",
    "                   'bundle_id': 'category',\n",
    "                   'correct_answer': 'category', \n",
    "                   'part': 'category',\n",
    "                   'tags': 'string'}\n",
    "            \n",
    "train_dtypes = {'content_id': 'category', \n",
    "                'content_type_id': 'category',\n",
    "                'task_container_id': 'category', \n",
    "                'user_answer': 'category',\n",
    "                'answered_correctly': 'boolean',\n",
    "                'prior_question_elapsed_time': 'float',\n",
    "                'prior_question_had_explanation': 'boolean'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Capstone 2 - Data Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
